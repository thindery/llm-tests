# LLM-Tests â€” Competitive Model Evaluation Framework

**Purpose:** A/B test coding capabilities between different LLM models on identical tasks

**Current Matchup:**
- ğŸ¤– **Blue Corner:** `ollama/kimi-k2.5:cloud`
- âš¡ **Red Corner:** `ollama/qwen3-coder-next:cloud` (NEW!)

---

## ğŸ¯ How It Works

### 1. The Challenge
- Each coding challenge is defined in `challenges/`
- Task description + acceptance criteria + test cases
- Example: "Build a React TODO app with localStorage persistence"

### 2. The Contenders
- **Each model gets its own branch** named: `model-{name}/task-{id}`
- Branches are completely isolated
- Same challenge, different model, same time limit

### 3. The Dev Agents
- **Dev A:** Assigned to kimi-k2.5 model
- **Dev B:** Assigned to qwen3-coder-next model
- Each implements the same task independently

### 4. The Review
- **Tech Lead** reviews both branches
- Compares: code quality, performance, readability, best practices
- **Declares a winner** with reasoning

### 5. The Record
- Results stored in `results.md`
- Track win/loss per model
- Build leaderboard over time

---

## ğŸ—ï¸ Repository Structure

```
llm-tests/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ challenges/               # Coding challenge definitions
â”‚   â”œâ”€â”€ 001-todo-app.md
â”‚   â”œâ”€â”€ 002-api-client.md
â”‚   â””â”€â”€ 003-form-validation.md
â”œâ”€â”€ src/                      # React app skeleton (main branch)
â”‚   â”œâ”€â”€ main.tsx
â”‚   â”œâ”€â”€ App.tsx
â”‚   â””â”€â”€ components/
â”œâ”€â”€ results/                  # Match results & analysis
â”‚   â”œâ”€â”€ 001-todo-app.md
â”‚   â””â”€â”€ leaderboard.md
â”œâ”€â”€ branches/                 # Branch documentation
â”‚   â”œâ”€â”€ model-kimi-k2-5/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ model-qwen3-coder/
â”‚       â””â”€â”€ README.md
â””â”€â”€ docs/                     # Process documentation
    â”œâ”€â”€ branching-strategy.md
    â”œâ”€â”€ review-criteria.md
    â””â”€â”€ model-configs.md
```

---

## ğŸŒ¿ Branch Naming Convention

Format: `model-{model-name}/task-{task-id}-{short-desc}`

**Examples:**
- `model-kimi-k2-5/task-001-todo-app`
- `model-qwen3-coder/task-001-todo-app`
- `model-kimi-k2-5/task-002-api-client`
- `model-qwen3-coder/task-002-api-client`

**Never merge these branches to main** â€” they're for comparison only.

---

## ğŸ¥Š Current Matchup: Match #001

| Blue Corner | Red Corner |
|-------------|------------|
| **Model:** ollama/kimi-k2.5:cloud | **Model:** ollama/qwen3-coder-next:cloud |
| **Branch:** `model-kimi-k2-5/task-001-todo-app` | **Branch:** `model-qwen3-coder/task-001-todo-app` |
| **Status:** ğŸŸ¡ Pending | **Status:** ğŸŸ¡ Pending |
| **Dev Agent:** Dev ğŸ‘¨â€ğŸ’» | **Dev Agent:** Dev ğŸ‘¨â€ğŸ’» |

**Challenge:** #001 â€” React TODO App with Persistence  
**Test Criteria:**
- âœ… Add TODO items
- âœ… Mark complete/incomplete
- âœ… Delete items
- âœ… Persist to localStorage
- âœ… Clean UI with Tailwind
- âš¡ Bonus: Animations

---

## ğŸ“ Process

### Step 1: Define Challenge
Create `challenges/001-task-name.md` with:
- Task description
- Acceptance criteria (bullet list)
- Bonus points
- Time limit (default: 1 hour)

### Step 2: Spawn Dev Agents
```
Spawn Dev A â†’ model=kimi-k2.5:cloud â†’ branch=model-kimi-k2-5/task-001
Spawn Dev B â†’ model=qwen3-coder-next:cloud â†’ branch=model-qwen3-coder/task-001
```

### Step 3: Review
Spawn Tech Lead to:
- Check out both branches
- Run both implementations
- Score on: correctness, code quality, UX, performance
- Declare winner with reasoning

### Step 4: Record
Update `results/001-task-name.md` with:
- Winner
- Scores (1-10 scale)
- Notable differences
- Learning for next match

---

## ğŸ† Leaderboard

Track cumulative wins:

| Model | Wins | Losses | Win Rate |
|-------|------|--------|----------|
| ollama/kimi-k2.5:cloud | 0 | 0 | â€” |
| ollama/qwen3-coder-next:cloud | 0 | 0 | â€” |

---

## ğŸ”® Future Matchups

Potential future candidates:
- Claude 3.5 Sonnet (via API)
- GPT-4o (via API)
- Gemini 2.5 Pro (via API)
- DeepSeek Coder (via Ollama)
- Phi-4 (via Ollama)
- Codestral (via Ollama)

Expand as we test more models!

---

## âš™ï¸ Model Configuration

### Current Models

**kimi-k2.5:cloud**
- Provider: Ollama Cloud
- Strengths: Strong reasoning, follows instructions
- Cost: $20/mo plan
- Context: Large

**qwen3-coder-next:cloud** â­ NEW
- Provider: Ollama Cloud
- Strengths: Latest coding-optimized model
- Cost: $20/mo plan (same)
- Notes: "Next-gen coder model" â€” claims better than Claude 3.5 Sonnet

---

**Created:** 2026-02-06  
**Next Match:** #001 â€” React TODO App  
**Status:** Ready to rumble! ğŸ¥ŠğŸ¦
